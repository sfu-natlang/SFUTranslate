reader:
    dataset:
        type: dummy_parallel # possible values [parallel | dummy_parallel | dummy_lm | mono]
        buffer_size: 1000
        max_length: 128 # for word-level it's better to be around 50-60, for bpe level around 128
        source_lang: fr
        target_lang: en
        working_dir: ../../../resources/iwslt_de_en/merge_tokenized
        train_file_name: train
        test_file_name: test
        dev_file_name: dev
        granularity:
            src: WORD # possible values ["WORD" (default) | "BPE" | "CHAR"]
            tgt: WORD # possible values ["WORD" (default) | "BPE" | "CHAR"]
        dummy:
            min_len: 8
            max_len: 50
            vocab_size: 96
            train_samples: 40000
            test_samples: 3000
            dev_samples: 1000
    vocab:
        bos_word: '<s>'
        eos_word: '</s>'
        pad_word: '<pad>'
        unk_word: '<unk>'
        space_word: '<space>'
        bpe_separator: '@@'
        bpe_merge_size:
            src: 30000
            tgt: 30000
        min_count:
            src: 2
            tgt: 2
    preprocess:
        dataset_directory: ../../../resources/iwslt_de_en
        result_directory: ../../../resources/iwslt_de_en/merge_tokenized
        source_lang: de
        target_lang: en
        dataset_type: IWSLT # possible values [WMT | IWSLT]
        to_lower: false
trainer:
    model:
        ####### universal configurations
        type: seq2seq # possible values [seq2seq | rnnlm | transformer | bytenet]
        bsize: 64 # size of the training sentence batches
        init_val: 0.1 # the value to range of which random variables get initiated
        best_model_path: null # the address of the best pre-trained model to be loaded before staring the train/test
        decoder_weight_tying: false # whether the weights need to be tied between the decoder embedding and generator
        beam_size: 1 # the size of the beams used in validation beam search, if set to 1 the search would be greedy search
        ####### seq2seq/rnnlm configurations
        tfr: 1.001 # teacher forcing ratio
        auto_tfr: false # if set to "true" the tfr value will be ignored and the teacher forcing ratio will decay with a ration of (1/numbe_of_epochs * (1-min_tfr)) per epoch
        min_tfr: 1.001 # if auto_tfr is set to true, the automatic tfr decay will begin from 1 and will get to this value after
        bienc: true # bidirectional encoding
        hsize: 256 # hidden state size of RNN layers
        nelayers: 1 # number of hidden layers in encoder
        ndlayers: 1 # number of hidden layers in decoder
        e_emb_size: 256
        d_emb_size: 256
        edropout: 0.3 # the dropout probability in the encoder
        ddropout: 0.3 # the dropout probability in the decoder
        decoder_attention_type: global # the type of attention used in the decoder, possible values [local|global]
        decoder_local_attention_d: 0.0 # the window span size of the local attention
        decoder_attention_method: dot # the attention type, possible values [add|general|concat|dot]
        ####### transformer configurations
        N: 6 # number of encoder/decoder layers
        d_model: 512 # size of each encoder/decoder layer
        d_ff: 2048 # size of intermediate layer in feedforward sub-layers
        h: 8 # number of heads
        dropout: 0.1 # the dropout used in encoder/decoder model parts
        smoothing: 0.1
        ####### bytenet configurations
        d: 400 # number of features in network
        max_r: 16 # max dilation size
        n_sets: 6 # number of ResBlock sets
        k: 3 # kernel size
    optimizer:
        name: adadelta
        lr: 1.0
        gcn: 5 # grad clip norm
        epochs: 15
        save_best_models: true
        early_stopping_loss: 0.01 # if the model reaches a loss below this value, the training will not continue anymore
        print_every_fraction: 0.25
        scheduler:
            name: cosine # possible values [cosine | step]
            eta_min: 1e-5 # used for "cosine" scheduler
            step_size: 5 # used for "step" scheduler
            gamma: 0.5 # used for "step" scheduler
        ####### transformer configurations
        warmup_steps: 400
        lr_update_factor: 1
        needs_warmup: false
        d_model: 256 # size of the model used for computing the warmup updates
    experiment:
        name: dummy