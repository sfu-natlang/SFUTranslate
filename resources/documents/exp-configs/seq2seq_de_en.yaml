# To run this experiment, you need to
#     0. put this file in /path/to/SFUTranslate/resources directory
#     1. download the English-German dataset from https://wit3.fbk.eu/mt.php?release=2017-01-trnted
#     2. extract it into a directory called iwslt in /path/to/SFUTranslate/resources
#     3. run "cd /path/to/SFUTranslate/src && python -m translate.reader.preprocess <name of this file placed in the resources directory>"
#     4. once the pre-processed data is ready in resources directory, you may run:
#         "cd /path/to/SFUTranslate/src && python -m translate.learning.trainer  <name of this file placed in the resources directory>"
#     5. The test Bleu score of the test set containing test201[0-5] is expected to be higher than 23 (we got 24.678 [greedy]) and the test loss should be below (2.600)

# You can download our pre-trained model from /cs/natlang-expts/hassan/SFUTranslate/pretrained/seq2seq_iwslt_de_en.pt

reader:
    dataset:
        type: parallel
        buffer_size: 10000
        max_length: 100
        source_lang: de
        target_lang: en
        working_dir: ../resources/iwslt_de_en/merge_lower_tokenized
        train_file_name: train
        test_file_name: test
        dev_file_name: dev
        granularity:
            src: BPE
            tgt: WORD
    vocab:
        bos_word: '<s>'
        eos_word: '</s>'
        pad_word: '<pad>'
        unk_word: '<unk>'
        space_word: '<space>'
        bpe_separator: '@@'
        bpe_merge_size:
            src: 30000
            tgt: 30000
        min_count:
            src: 3
            tgt: 2
    preprocess:
        dataset_directory: ../../../resources/iwslt_de_en
        result_directory: ../../../resources/iwslt_de_en/merge_lower_tokenized
        source_lang: de
        target_lang: en
        to_lower: true
        dataset_type: IWSLT
trainer:
    model:
        type: seq2seq
        bsize: 64
        init_val: 0.1
        best_model_path: null
        beam_size: 1
        tfr: 1.001
        min_tfr: 0.6
        auto_tfr: true
        bienc: true
        hsize: 512
        nelayers: 1
        ndlayers: 1
        e_emb_size: 512
        d_emb_size: 512
        ddropout: 0.2
        decoder_attention_type: global
        decoder_local_attention_d: 0.0
        decoder_attention_method: general
    optimizer:
        name: sgd
        lr: 1.0
        gcn: 0.25
        epochs: 15
        save_best_models: true
        early_stopping_loss: 0.01
        print_every_fraction: 0.25
    experiment:
        name: seq2seq_iwslt_de_en
