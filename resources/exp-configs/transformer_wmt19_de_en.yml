debug_mode: false
src_lang: de
tgt_lang: en
dataset_name: wmt19_de_en
dataset_is_in_bpe: true
lowercase_data: true
pad_token: <pad>
bos_token: <bos>
eos_token: <eos>
unk_token: <unk>
propn_token: <propn>
max_sequence_length: 80
max_vocab_src: 50000
min_freq_src: 2
max_vocab_tgt: 50000
min_freq_tgt: 1
extract_unk_stats: false

emb_dropout: 0.1
train_batch_size: 4096
valid_batch_size: 256
encoder_emb_size: 600
encoder_hidden_size: 600
encoder_layers: 1
encoder_dropout_rate: 0.1
decoder_emb_size: 600
decoder_hidden_size: 1200
decoder_layers: 1
decoder_dropout_rate: 0.1
out_dropout: 0.2
coverage_dropout: 0.2

model_name: transformer
transformer_d_model: 512
transformer_h: 8
transformer_dropout: 0.1
transformer_d_ff: 2048
transformer_max_len: 4096
transformer_N: 6
transformer_loss_smoothing: 0.1
transformer_opt_factor: 1
transformer_opt_warmup: 4000

maximum_decoding_length: 100
bahdanau_attention: true
coverage_required: true
coverage_lambda: 1.0

n_epochs: 6
init_optim: adagrad
init_learning_rate: 0.01
init_epochs: 0
optim: adam
learning_rate: 0.001
learning_momentum: 0.9
grad_clip: true
max_grad_norm: 1.0
val_slices: 10
lr_decay_patience_steps: 5
lr_decay_factor: 0.9
lr_decay_threshold: 0.1
lr_decay_min: 0.00001

beam_size: 5
beam_search_length_norm_factor: 0.65
beam_search_coverage_penalty_factor: 0.4
checkpoint_name: transformer_wmt19_de_en.pt
