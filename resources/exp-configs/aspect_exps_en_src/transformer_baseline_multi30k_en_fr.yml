debug_mode: false
src_lang: en
tgt_lang: fr
dataset_name: multi30k16
lowercase_data: false
src_tokenizer: bert
tgt_tokenizer: bert
pad_token: <pad>
bos_token: <bos>
eos_token: <eos>
unk_token: <unk>
propn_token: <propn>
max_sequence_length: 100
max_vocab_src: 50000
min_freq_src: 0
max_vocab_tgt: 50000
min_freq_tgt: 0
extract_unk_stats: false
share_vocabulary: true
sentence_count_limit: -1

emb_dropout: 0.3
train_batch_size: 2560
valid_batch_size: 256
encoder_emb_size: 250
encoder_hidden_size: 250
encoder_layers: 1
encoder_dropout_rate: 0.1
decoder_emb_size: 250
decoder_hidden_size: 500
decoder_layers: 1
decoder_dropout_rate: 0.1
out_dropout: 0.2
coverage_dropout: 0.2

model_name: transformer
transformer_d_model: 256
transformer_h: 8
transformer_dropout: 0.1
transformer_d_ff: 512
transformer_max_len: 2560
transformer_N: 4
transformer_loss_smoothing: 0.1
transformer_opt_factor: 1
transformer_opt_warmup: 2000
share_all_embeddings: true

maximum_decoding_length: 100
bahdanau_attention: true
coverage_required: true
coverage_lambda: 1.0

n_epochs: 20
init_optim: adagrad
init_learning_rate: 0.01
init_epochs: 0
optim: adam
learning_rate: 0.001
learning_momentum: 0.9
grad_clip: false
max_grad_norm: 1.0
val_slices: 5
lr_decay_patience_steps: 5
lr_decay_factor: 0.9
lr_decay_threshold: 0.1
lr_decay_min: 0.00001
update_freq: 1

beam_size: 4
beam_search_length_norm_factor: 0.6
beam_search_coverage_penalty_factor: 0.4
checkpoint_name: transformer_baseline_multi30k_en_fr.pt
